# ML - Architectures
|Title|Type|Conference|Remarks
|--|--|--|--|
|[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|Paper|NIPS|Transformer architecture uses attention mechanism to replace the auto-regressive way to encode sentence by RNN-based architecture. Allow for parallelization for training, inference is still autoregressive. Introduce positional embedding and layerwise normalization as well. Note that original paper's layer norm is different from many up-to-date implementation. |
|[SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1802.05957.pdf)|Paper|ICML|on reading plan


[Back to index](../README.md)