# NLP - Pretrained Language Models
|Title|Type|Conference|Remarks
|--|--|--|--|
|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)|Paper|NAACL|Transformer-based Pretrain Model. Instead of using autoregressive way to train LM, use masks (Masked Language Model) to pretrain the model along with Next senetence prediction task. Later variations of BERT like XLNet add in permutation for MLM task to address the problem that masked words cannot infer information from each other. 
|[XLNet]|x|x|x|
|[ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)|x|x|x|
[Back to index](../README.md)