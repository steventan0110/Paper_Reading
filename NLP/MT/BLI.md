# NLP - Bilingual Lexicon Induction
|Title|Type|Conference|Remarks
|--|--|--|--|
|[VecMap (cross-lingual word embedding mappings)](https://github.com/artetxem/vecmap)|Papers|Several Conferences|A series of paper by Mikel Artetxe that uses variation of VecMap to learn BLI: [Learning principled bilingual mappings of word embeddings while preserving monolingual invariance](https://aclanthology.org/D16-1250.pdf): investigate the effect of orthogonal mapping (transformation is orthogonally contrained), as well as length normalization and mean centering. [Learning bilingual word embeddings with (almost) no bilingual data](https://aclanthology.org/P17-1042.pdf): Use previous result iteratively and show superior results on small seed.[Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations](http://ixa.si.ehu.es/sites/default/files/dokumentuak/11455/aaai18.pdf): Learn orthogonal mapping in several steps to gain interpretability. Summarize previous works using 5 steps (whitening, re-weighting, etc.) [A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings](https://aclanthology.org/P18-1073.pdf) VecMap paper (zero seed ), use self-learning to improve dictionary and improve mappin. Added some tricks to make initialization and retrieval better, such as CSLS (basically the margin score for cosine similarity). |
|[Density Matching for Bilingual Word Embedding](https://arxiv.org/pdf/1904.02343.pdf)|Paper|NAACL2019| An approach that expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. Additional training objective like weakly-supervision and orthogonal constraint are added.|
|[A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction](https://aclanthology.org/2020.acl-main.318.pdf)|Paper|ACL2020|An approach that extracts word cliques from the graphs (with BK algorithm and pivoting) and map the cliques of two languages to get initial mapping (use clique embedding). Use this as seed dictionary to induce BLI (following SVD decomposition method before but use a groupof k W's instead of an universal one).|
|[Adversarial Training for Unsupervised Bilingual Lexicon Induction](https://aclanthology.org/P17-1179.pdf)|Paper|ACL2017|Apply adversarial methods (GAN, auto-encoder) to learn seed dictionary with orthogonal parametrization.|
|[Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction](https://aclanthology.org/2020.acl-main.618.pdf)|Paper|ACL2020|Iteratively improve seed dictionary with self-learning procedure. Use multi-layer perceptron to train a classifier that incorporates several translation level signal to help score the selected additional dictionary seeds|
|[WORD TRANSLATION WITHOUT PARALLEL DATA](https://openreview.net/pdf?id=H196sainb)|Paper|ICLR2018|Follow GAN to unsupervisely learn the mapping. Proposed CLSL method to address hubness problem|
|[Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction](https://arxiv.org/pdf/2105.12297.pdf)|Paper|ArXiv|Modify the SVD decomposition solution for Procrustes problem using orthogonal rotation matrix and cross-lingual scaling to resulted embedding|
|[A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction](https://aclanthology.org/2020.coling-main.526.pdf)|Paper|COLING2020|Domain Adaptation applied (Use PCA before each init step of VecMap) to get more robustness BLI result, especially on low resource datasets.|
|[Bilingual Lexicon Induction through Unsupervised Machine Translation](https://arxiv.org/pdf/1907.10761.pdf)|Paper|ACL2019| Build unsupervised  MT systems with fastText, VecMap, and phrase-based n-gram table. Then create synthetic parallel data and retrieve new dictionary from there.|
|[An Effective Approach to Unsupervised Machine Translation](https://aclanthology.org/P19-1019.pdf)|Paper|ACL2019|Build on previous SMT-based unsupervised MT system with cyclic loss, dual system, as well as length penalty.|
|[When Does Unsupervised Machine Translation Work?](https://arxiv.org/pdf/2004.05516.pdf)|Paper|WMT2020|Evaluate Artetxe's unsupervised SMT+NMT system for schemes including dissimilar languages, dissimilar source and target domains, diverse datasets, and authentic low-resource language pairs?
|[Are All Good Word Vector Spaces Isomorphic?](https://aclanthology.org/2020.emnlp-main.257.pdf)|Paper|EMNLP 2020|Near-isomorphism arises only with sufficient amounts of training. Quantify isomorphism with Eigenvector similarity, GH distance, and Relational similarity. Modify training data to simulate low-resource condition.|
|[Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation](https://aclanthology.org/2021.naacl-main.16.pdf)|Paper|NAACL2021|Use pretrained VecMap BLI to init MLM training for unsupervised MT and achive state of the art on Mk, Sq languages.|
|[Leveraging Vector Space Similarity for Learning Cross-Lingual Word Embeddings: A Systematic Review](https://www.mdpi.com/2673-6470/1/3/11/htm)|Paper|MDPI|Review paper|
|[Unsupervised Multilingual Word Embeddings](https://arxiv.org/pdf/1808.08933.pdf)|Paper|EMNLP2018|Multilingual Adversarial Training to map two language embeddings into the same space|
|[Knowledge Distillation for Bilingual Dictionary Induction](https://aclanthology.org/D17-1264.pdf)|Paper|EMNLP2017|Apply Knowledge distillation on BLI task with pivot language|
|[UNSUPERVISED HYPERALIGNMENT FOR MULTILINGUAL WORD EMBEDDINGS](https://arxiv.org/pdf/1811.01124.pdf)|Paper|ICLR2019|Add hyperalignment supervision. Instead of constraining the mappings directly, their formulation constrains every pair of vectors so that they are aligned well|
|[Unsupervised Multilingual Alignment using Wasserstein Barycenter](https://arxiv.org/pdf/2002.00743.pdf)|Paper|IJCAI2020|Use Wasserstein barycenter to determine the pivot language|
|[Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages](https://aclanthology.org/D19-1076.pdf)|Paper|EMNLP2019|Use a noise tolerant piecewise
linear technique to learn a non-linear mapping between two monolingual word embedding vector spaces. (Supervised approach on BLI)|
|[How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions](https://arxiv.org/pdf/1902.00508.pdf)|Paper|ACL2019|Evaluation analysis of BLI for downstream tasks such as NLI, Document classification, information retrieval|




[Back to index](../../README.md)