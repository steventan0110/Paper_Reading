# NLP - Bilingual Lexicon Induction
|Title|Type|Conference|Remarks
|--|--|--|--|
|[VecMap (cross-lingual word embedding mappings)](https://github.com/artetxem/vecmap)|Papers|Several Conferences|A series of paper by Mikel Artetxe that uses variation of VecMap to learn BLI: [Learning principled bilingual mappings of word embeddings while preserving monolingual invariance](https://aclanthology.org/D16-1250.pdf): investigate the effect of orthogonal mapping (transformation is orthogonally contrained), as well as length normalization and mean centering. [Learning bilingual word embeddings with (almost) no bilingual data](https://aclanthology.org/P17-1042.pdf): Use previous result iteratively and show superior results on small seed.[Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations](http://ixa.si.ehu.es/sites/default/files/dokumentuak/11455/aaai18.pdf): Learn orthogonal mapping in several steps to gain interpretability. Summarize previous works using 5 steps (whitening, re-weighting, etc.) [A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings](https://aclanthology.org/P18-1073.pdf) VecMap paper (zero seed ), use self-learning to improve dictionary and improve mappin. Added some tricks to make initialization and retrieval better, such as CSLS (basically the margin score for cosine similarity). |
|[Density Matching for Bilingual Word Embedding](https://arxiv.org/pdf/1904.02343.pdf)|Paper|NAACL2019| An approach that expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. Additional training objective like weakly-supervision and orthogonal constraint are added.|
|[A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction](https://aclanthology.org/2020.acl-main.318.pdf)|Paper|ACL2020|An approach that extracts word cliques from the graphs (with BK algorithm and pivoting) and map the cliques of two languages to get initial mapping (use clique embedding). Use this as seed dictionary to induce BLI (following SVD decomposition method before but use a groupof k W's instead of an universal one).|
|[Adversarial Training for Unsupervised Bilingual Lexicon Induction](https://aclanthology.org/P17-1179.pdf)|Paper|ACL2017|Apply adversarial methods (GAN, auto-encoder) to learn seed dictionary with orthogonal parametrization.|
|[Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction](https://aclanthology.org/2020.acl-main.618.pdf)|Paper|ACL2020|Iteratively improve seed dictionary with self-learning procedure. Use multi-layer perceptron to train a classifier that incorporates several translation level signal to help score the selected additional dictionary seeds|
|[WORD TRANSLATION WITHOUT PARALLEL DATA](https://openreview.net/pdf?id=H196sainb)|Paper|ICLR2018|Follow GAN to unsupervisely learn the mapping. Proposed CLSL method to address hubness problem|
|[Word Embedding Transformation for Robust Unsupervised Bilingual Lexicon Induction](https://arxiv.org/pdf/2105.12297.pdf)|Paper|ArXiv|Modify the SVD decomposition solution for Procrustes problem using orthogonal rotation matrix and cross-lingual scaling to resulted embedding|
|[A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction](https://aclanthology.org/2020.coling-main.526.pdf)|Paper|COLING2020|Domain Adaptation applied (Use PCA before each init step of VecMap) to get more robustness BLI result, especially on low resource datasets.|
|[Bilingual Lexicon Induction through Unsupervised Machine Translation](https://arxiv.org/pdf/1907.10761.pdf)|Paper|ACL2019| Build unsupervised  MT systems with fastText, VecMap, and phrase-based n-gram table. Then create synthetic parallel data and retrieve new dictionary from there.|
|[An Effective Approach to Unsupervised Machine Translation](https://aclanthology.org/P19-1019.pdf)|Paper|ACL2019|Build on previous SMT-based unsupervised MT system with cyclic loss, dual system, as well as length penalty.|
|[When Does Unsupervised Machine Translation Work?](https://arxiv.org/pdf/2004.05516.pdf)|Paper|WMT2020|Evaluate Artetxe's unsupervised SMT+NMT system for schemes including dissimilar languages, dissimilar source and target domains, diverse datasets, and authentic low-resource language pairs?




[Back to index](../../README.md)