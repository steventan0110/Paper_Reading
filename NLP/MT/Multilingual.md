# Low resource and Multilingual Translation
|Title|Type|Conference|Remarks
|--|--|--|--|
|[Survey of Low-Resource Machine Translation](https://arxiv.org/pdf/2109.00486.pdf)|Paper|ArXiv|Comprehensive survey paper on current research for low resource translation. Personally care most about transfer learning in multilingual setting.
|[Transfer Learning for Low-Resource Neural Machine Translation](https://aclanthology.org/D16-1163.pdf)|Paper|EMNLP|Use parent model trained on Fr-En to help transfer knowledge.|
|[Choosing Transfer Languages for Cross-Lingual Learning](https://aclanthology.org/P19-1301.pdf)|Paper|ACL|Train multiple transfer learning models based on variuos consideration and rank the models. For MT, data-dependent features such as relative type-token ratios is most useful.|
|[Massively Multilingual Neural Machine Translation](https://arxiv.org/pdf/1903.00089.pdf)|Paper|NAACL|Experimented on 103 languages and gained improvement on low resource translation with massive multilingual model.|
|[Universal Neural Machine Translation for Extremely Low Resource Languages](https://aclanthology.org/N18-1032.pdf)|Paper|ACL|Incorporate Universal Lexical Representation (ULR, maps word from its own embedding to an univseral embedding, which in this case, is based on En) and Mixure of Language Experts into multilingual translation for low resource languages.|
|[Improving Lexical Choice in Neural Machine Translation](https://aclanthology.org/N18-1031.pdf)|Paper|ACL|Propose methods to regularize the embedding output to be less biased on frequent words and use lexical module to make model less focused on context but on direct translation.|
|[Iterative Back-Translation for Neural Machine Translation](https://aclanthology.org/W18-2703.pdf)|ACL-WMT|Iterative backtranslation for high and low resource language gain improvement|



[Back to index](../../README.md)
